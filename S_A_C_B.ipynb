{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Experience_Replay.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, LSTM\n",
    "import import_ipynb\n",
    "import Experience_Replay\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#physical_device = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_device[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 3e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256,  name='criticB', chkpt_dir='tmpB/sacB'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.model_name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sacB')\n",
    "        \n",
    "        self.fc1 = Dense(self.fc1_dims, activation=LeakyReLU())\n",
    "        self.fc2 = Dense(self.fc2_dims, activation=LeakyReLU())\n",
    "        self.q = Dense(1, activation=None)\n",
    "    def call(self, state, action):\n",
    "        action_value = self.fc1(tf.concat([state, action], axis=1))\n",
    "        action_value = self.fc2(action_value)\n",
    "      \n",
    "        \n",
    "        q = self.q(action_value)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "\n",
    "class ValueNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256,  name='valueB', chkpt_dir='tmpB/sacB'):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.model_name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sacB')\n",
    "        \n",
    "        self.fc1 = Dense(self.fc1_dims, activation=LeakyReLU())\n",
    "        self.fc2 = Dense(self.fc2_dims, activation=LeakyReLU())\n",
    "\n",
    "        self.v = Dense(1, activation=None)\n",
    "    def call(self, state):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.fc2(state_value)\n",
    "     \n",
    "        \n",
    "        v = self.v(state_value)\n",
    "        \n",
    "        return v\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, max_action, fc1_dims=256, fc2_dims=256, n_actions = 1,  name='actorB', chkpt_dir='tmpB/sacB'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.max_action = max_action\n",
    "        self.model_name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sacB')\n",
    "        self.noise = 1e-6 #1e-2 or 0.01\n",
    "        \n",
    "        self.fc1 = Dense(self.fc1_dims, activation=LeakyReLU())\n",
    "        self.fc2 = Dense(self.fc2_dims, activation=LeakyReLU())\n",
    "      \n",
    "        \n",
    "        self.mu =  Dense(self.n_actions, activation=None)\n",
    "     \n",
    "        self.sigma = Dense(self.n_actions, activation=None)\n",
    "    def call(self, state):\n",
    "        prob = self.fc1(state)\n",
    " \n",
    "        prob = self.fc2(prob)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        mu = self.mu(prob)\n",
    "        sigma = self.sigma(prob)\n",
    "\n",
    "        sigma = tf.clip_by_value(sigma, self.noise, 1)\n",
    "        \n",
    "        return mu, sigma\n",
    "    def sample_normal(self, state):\n",
    "        mu, sigma = self.call(state)\n",
    "        #sigma = 1e-6\n",
    "        \n",
    "        probabilities = tfp.distributions.Normal(mu, sigma)\n",
    "    \n",
    "        actions =  probabilities.sample()\n",
    "      \n",
    "     \n",
    "        action = tf.math.tanh(actions)*self.max_action\n",
    "  \n",
    "        \n",
    "    \n",
    "        \n",
    "        log_probs = probabilities.log_prob(actions)\n",
    "     \n",
    "        \n",
    "        log_probs = log_probs - tf.math.log(1-tf.math.pow(action, 2)+self.noise)\n",
    "\n",
    "        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n",
    "      \n",
    "        \n",
    "        return action, log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentB:\n",
    "    def __init__(self, alpha=lr_schedule, beta=lr_schedule, input_dims=[12], env=None,\n",
    "                gamma=0.99, n_actions=3, max_size=1000000, tau=0.005, layer1_size=256 \n",
    "                , layer2_size = 256, batch_size=256, reward_scale =1):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = Experience_Replay.EP(max_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.actor= ActorNetwork(n_actions=n_actions,\n",
    "                                max_action=1)\n",
    "        self.critic_1 = CriticNetwork(n_actions=n_actions)\n",
    "        self.critic_2 = CriticNetwork(n_actions=n_actions)\n",
    "        self.value = ValueNetwork(n_actions=n_actions)\n",
    "        self.target_value = ValueNetwork(n_actions=n_actions)\n",
    "        \n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate= beta))\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate= beta))\n",
    "        self.value.compile(optimizer=Adam(learning_rate= beta))\n",
    "        self.target_value.compile(optimizer=Adam(learning_rate= beta))\n",
    "        \n",
    "        self.scale = reward_scale\n",
    "        self.update_network_parameters(tau=1)\n",
    "    def choose_action(self, observation, epi_time, start):\n",
    "        if start == True:\n",
    "            s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15,s16,s17,s18 = observation[0]\n",
    "            \n",
    "           \n",
    "            observation = np.array([s1[0], s2[0],s3[0]/1000,s4[0],s5[0],s6[0]/1000,s7,s8,s9,s10[0]/1000,s11[0],s12[0],s13[0],s14[0],s15[0],s16[0],s17[0],s18[0]])\n",
    "            \n",
    "        else:\n",
    "            s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15,s16,s17,s18 = observation[0]\n",
    "            \n",
    "          \n",
    "            observation = np.array([s1[epi_time], s2[epi_time],s3[epi_time]/1000,s4[epi_time],s5[epi_time],s6[epi_time]/1000,s7,s8,s9,s10[epi_time]/1000,s11[epi_time],s12[epi_time],s13[epi_time],s14[epi_time],s15[epi_time],s16[epi_time],s17[epi_time],s18[epi_time]])\n",
    "                       \n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        \n",
    "        actions,l = self.actor.sample_normal(state)\n",
    "        return actions[0]\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        #self.memory.update(state, action, reward, new_state, done)\n",
    "        self.exp_1 = self.memory.update(state, action, reward, new_state, done)\n",
    "        self.done = done\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau= self.tau\n",
    "        weights = []\n",
    "        targets = self.target_value.weights\n",
    "        for i, weight in enumerate(self.target_value.weights):\n",
    "            weights.append(weight*tau + targets[i]*(1-tau))\n",
    "        self.target_value.set_weights(weights)\n",
    "        \n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_weights(self.actor.checkpoint_file)\n",
    "        self.critic_1.save_weights(self.critic_1.checkpoint_file)\n",
    "        self.critic_2.save_weights(self.critic_2.checkpoint_file)\n",
    "        self.value.save_weights(self.value.checkpoint_file)\n",
    "        self.target_value.save_weights(self.target_value.checkpoint_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_weights(self.actor.checkpoint_file)\n",
    "        self.critic_1.load_weights(self.critic_1.checkpoint_file)\n",
    "        self.critic_2.load_weights(self.critic_2.checkpoint_file)\n",
    "        self.value.load_weights(self.value.checkpoint_file)\n",
    "        self.target_value.load_weights(self.target_value.checkpoint_file)\n",
    "\n",
    "    \n",
    "    def learn(self,aplh):\n",
    "        exp_length = len(self.exp_1)\n",
    "        if exp_length < 10000: \n",
    "            return\n",
    "        #print('Learning.......')\n",
    "        self.batch_train = self.memory.sample(self.batch_size)\n",
    "        state = []\n",
    "        action = []\n",
    "        reward = []\n",
    "        new_state = []\n",
    "        done = []\n",
    "        for i in range(len(self.batch_train)):\n",
    "            state.append(self.batch_train[i][0])\n",
    "            action.append(self.batch_train[i][1])\n",
    "            reward.append(self.batch_train[i][2])\n",
    "            new_state.append(self.batch_train[i][3])\n",
    "            done.append(self.batch_train[i][4])\n",
    "        \n",
    "        \n",
    "        state =np.asarray(state, dtype=np.float32)\n",
    "        action =np.asarray(action, dtype=np.float32)\n",
    "        reward = np.asarray(reward, dtype=np.float32)\n",
    "        new_state =np.asarray(new_state, dtype=np.float32)\n",
    "        done =np.asarray(done, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            value = tf.squeeze(self.value(states), 1)\n",
    "           \n",
    "            value_ = tf.squeeze(self.target_value(states_), 1)\n",
    "            \n",
    "            current_policy_actions, log_probs = self.actor.sample_normal(states)\n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            q1_new_policy = self.critic_1(states, current_policy_actions)\n",
    "            q2_new_policy = self.critic_1(states, current_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy, q2_new_policy), 1)\n",
    "            value_target =critic_value-(aplh*log_probs)#log_probs(aplh*-2)\n",
    "            value_loss = 0.5* keras.losses.MSE(value, value_target)\n",
    "           \n",
    "        value_network_gradient = tape.gradient(value_loss, self.value.trainable_variables)\n",
    "            \n",
    "        self.value.optimizer.apply_gradients(zip(value_network_gradient, self.value.trainable_variables ))\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            new_policy_actions, log_probs = self.actor.sample_normal(states)\n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            q1_new_policy = self.critic_1(states, new_policy_actions)\n",
    "            q2_new_policy = self.critic_1(states, new_policy_actions)  \n",
    "            \n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy, q2_new_policy), 1)\n",
    "            \n",
    "            actor_loss = (aplh*log_probs) -critic_value #log_probs(aplh*-2)\n",
    "            #print(tf.math.reduce_mean(log_probs/critic_value))\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "            print(tf.math.reduce_mean(log_probs))\n",
    "            \n",
    "        actor_network_gradient =  tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        \n",
    "        self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables ))\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            \n",
    "            q_hat = self.scale*rewards + self.gamma*value_*(1-int(0)) #entropy term\n",
    "            print(tf.math.reduce_mean(rewards))\n",
    "            print(tf.math.reduce_mean(value_))\n",
    "            q1_old_policy = tf.squeeze(self.critic_1(state, action), 1)\n",
    "            q2_old_policy = tf.squeeze(self.critic_2(state, action), 1)\n",
    "           \n",
    "            critic_1_loss = 0.5*keras.losses.MSE(q1_old_policy, q_hat)\n",
    "            critic_2_loss =  0.5*keras.losses.MSE(q2_old_policy, q_hat)\n",
    "          \n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n",
    "        critic_2_network_gradient = tape.gradient(critic_2_loss, self.critic_2.trainable_variables)\n",
    "            \n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_2_network_gradient, self.critic_2.trainable_variables))\n",
    "        \n",
    "        self.update_network_parameters()\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
